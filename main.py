from dotenv import load_dotenv

load_dotenv()
from typing import Set

import streamlit as st
from streamlit_chat import message

from backend.core import run_llm

# Add sidebar with user information
with st.sidebar:
    st.title("User Profile")
    if st.experimental_user.email:
        st.image("https://www.gravatar.com/avatar/00000000000000000000000000000000?d=mp&f=y", width=100)
        st.write(f"Name: {st.experimental_user.email.split('@')[0]}")
        st.write(f"Email: {st.experimental_user.email}")
    else:
        st.write("Please log in to view your profile.")

st.header("LangChain - Documentation Helper Bot")

prompt = st.text_input("Prompt", placeholder="Enter your prompt here..")
# When the user enters something and clicks 'enter', the value of the entered string goes into the 'prompt'.

if (
    "chat_answers_history" not in st.session_state
    and "user_prompt_history" not in st.session_state
    and "chat_history" not in st.session_state
):
    st.session_state["chat_answers_history"] = [] # st.session_state["chat_answers_history"] is a list of strings.
    st.session_state["user_prompt_history"] = [] # st.session_state["user_prompt_history"] is a list of strings.
    st.session_state["chat_history"] = [] # st.session_state["chat_history"] is a list of tuples.


def create_sources_string(source_urls: Set[str]) -> str:
    if not source_urls:
        return ""
    sources_list = list(source_urls)
    sources_list.sort()
    # The above line sorts the html links of source documents in alphabetic order.
    sources_string = "sources:\n"
    for i, source in enumerate(sources_list):
        sources_string += f"{i+1}. {source}\n"
    return sources_string

if prompt:
    with st.spinner("Generating response.."):
        generated_response = run_llm(query=prompt, chat_history=st.session_state["chat_history"])
        sources = set(
            [doc.metadata["source"] for doc in generated_response["context"]]
        )
        # 'sources' is a 'set' of html url links of the retrieved context documents.
        # Html link A may contain context 1 doc, context 2 doc, context 3 doc
        # Html link B may contain context 4 doc, context 5 doc, context 6 doc
        # Suppose the retrieved context docs are context 2 doc and context 4 doc,
        # then 'sources' = set(Html link A, Html link B)

        formatted_response = (
            f"{generated_response['answer']} \n\n {create_sources_string(sources)}"
        )

        st.session_state["user_prompt_history"].append(prompt)
        st.session_state["chat_answers_history"].append(formatted_response)
        st.session_state["chat_history"].append(("human", prompt))
        st.session_state["chat_history"].append(("ai", generated_response["answer"]))
        # For the persistence of values
        # The first element of tuple is the 'role' and the second element of tuple is the user_prompt or
        # answer generated by llm.

if st.session_state["chat_answers_history"]:
    for generated_response, user_query in zip(
            st.session_state["chat_answers_history"],
            st.session_state["user_prompt_history"],
    ):
        message(user_query, is_user=True)
        # 'is_user = True' makes the sure the avatar/icon of the message will be that of the user.
        message(generated_response)